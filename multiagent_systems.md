multiagent_systems
==================
- Category: Learning
- Tags: 
- Created: 2025-04-26T14:48:25-07:00

## CHAPTER 1 - DISTRIBUTED CONSTRAINT SATISFACTION

### 1.1 - Defining distributed CSP
- Multiagent systems are common. Here's a good example: distributed constraint satisfaction
	- Given a set of variables and a set of constraints between variables, how do we assign these variables to satisfy the constraints?
- Useful way to visualize this - graph coloring problem.
	- Three nodes connected to one another have the constraints that they cannot be colored the same as other nodes.
		- Each can be red, blue, or green
- How do we formally define a CSP?
	- A CSP consists of:
		- Finite set of variables $X = {X_1, X_2, ..., X_n}$
			- Ex. $X = {X_1, X_2, X_3}$
		- Domain $D_i$ for each variable $X_i$
			- Ex. $D_1 = {red, blue, green}, D_2 = ...$
		- Set of constraints ${C_1, C_2, ..., C_m}$
			- Constraints are predicated on some subset of the variables $X$
			- Contraints restrict the participating variables
			- Ex. `!(C_1 = red && C_2 = red)`
	- *Instantiation* of variable subset $S \in X$ = assignment of a unique domain value for each variable in $S$
		- It's a *legal instantiation* if all constraints which only mention variables in $S$ are satisfied
		- A *solution* to a network is a legal instantiation of all variables
- In distributed CSP:
	- Each variable is controlled by an independent agent
	- Agents can (generally) assign their own variable
	- Agents can (generally) only communicate with neighbors in the constraint graph
	- Agents follow a distributed algorithm - local computation + communication with other agents = organization!
		- Good algorithms find (or disprove) a solution quickly
		- We examine two types of algorithms:
			- Least-commitment approach
			- Heuristic approach
	- Assume for all problems that messages arrive on time and in order

### 1.2 - Domain pruning algorithms
- Consider: domain-pruning algorithms
	- We start with the Revise function
		- "For every value $v_i$ in my domain $D_i$, if my neighbor has no value $v_j$ in $D_j$ with which our constraint is satisfied, remove $v_i$ from $D_i$"
		- Also known as "arc consistency"
	- If this function terminates with one solution per node, we have a solution
	- If it terminates with more than one solution per node, we *might* have a solution - inconclusive
	- If it terminates when a node has no values in domain, there is no solution
	- (!) The algorithm is sound (always correct) and guaranteed to terminate, but is not complete (may fail to produce a verdict)
		- See default red/green/blue algorithm - this algorithm can be used for preprocessing, but is very weak
	- Based on *unit resolution* from propositional logic
		- "If $A_1$ (is true) and $\neg(A_1 \land A_2 \land ... \land A_n)$, then $\neg(A_2 \land A_3 \land ... \land A_n)$"
		- Translate this to the situation:
		- "If they can only be red and we can't both be red, then I can't be red."
			- A *Nogood* is a rule detailing a forbidden value combination.
			- We write $\neg(x_1 = red \land x_2 = red)$ as the Nogood ${x_1, x_2}$
	- This is a weak rule, though. We need something bigger: *hyper-resolution*
		- Essentially the same as resolution, but for all possibilities of the leading variable at once.
		- See paper for examples, but here's an example flow:
			- Node $i$ can be $v_1 \lor v_2 \lor ... \lor v_n$
			- We also have a set of Nogoods, one for each $v_n$
			- We hyperresolve these combinations to generate a new Nogood (like in unit resolution,) if it doesn't already exist in the node's set
- Using hyper-resolution, consider the following algorithm: ReviseHR
	- Following flow:
		- "I am agent $i$ and I have a set of Nogoods $NG_i$. I also just got a set of Nogoods from agent $j$ called $NG^*_j"
		- "I'll add any Nogoods from $NG^*_j$ to $NG_i$ if I didn't already have them"
		- "Now, I'll hyper-resolve and try to generate any new Nogoods from my new set. Collect these in $NG^*_i$"
		- "If my set **is nonempty**, then add $NG^*_i$ to $NG_i$ and send it out to my neighbors."
			- If I produce no Nogoods, I didn't receive any new information. This is fine, but since $NG_i$ presumably changed, keep working.
			- Producing nothing is **NOT** the same as producing an empty set. Empty set means there is no logical solution to satisfy the problem.
		- "If I find the empty set in my generated set of Nogoods, then there's no logically no solution and I should stop working."
			- I *want* to send the empty set, but stop myself from continuing after.
		- "Repeat this flow until there's no more changes in my set of Nogoods $NG_i$"
	- See red/blue exclusive nodes example - generate Nogoods sequentially until there are no more options, then deduce an empty set
- This algorithm isn't the most practical - we have to send massive amounts of Nogoods. Neither was the Revise function. Why?
	- Because they're *least commitment* - they are restricted to removing only provably impossible value combinations.
	- Instead, we can use heuristics. Try out values, and backtrack when they don't work.

### 1.3 - Heuristic search algorithms
- Name of the game - try out certain values, communicate between nodes and backtrack when needed.
- Consider a centralized heuristic algorithm
	- Central controller orders nodes, tries out values in a row, backtrack earlier in the row when stuff doesn't work.
	- This is nice, but not distributed.
- Consider a (naive) distributed heuristic algorithm
	- Each node tries out a value and notifies neighbors of value. If not consistent with constraints, switch and notify.
	- This is alright, but it might cycle forever, even with a solution
- Let's combine these two (flawed) algorithms into something nicer. Specifically, let's include:
	- The ordered structure of nodes from alg. 1
	- The messaging algorithm from alg. 2
- Result: **asynchronous backtracking algorithm (ABT)**
	- Key notes:
		- Each node communicates its value updates to *lower* priority nodes ONLY
		- When a node receives a value from a higher node, it checks for its own congruency.
			- If it doesn't need to move, it doesn't move
			- If it needs to move and is able to find a new spot, it moves and sends another update to lower nodes
			- If it needs to move and is unable to find a new spot, it deletes the value of the next highest node from its agent view, moves, updates lower nodes, and sends a Nogood with the (pruned) agent view to the next highest node (value previously deleted from view.)
		- When a node receives a Nogood from a lower node, it records it and attempts to find a new spot. See above steps.
	- Example: $n$ queens problem. Try running through it!
	- ABT can be optimized for sure. Mostly revolves around Nogoods being simplified.
		- Finding an absolutely minimal Nogood is NP-hard. Use various heuristics to cut down size without being wrong.

## CHAPTER 2 - DISTRIBUTED OPTIMIZATION

### 2.1 - Distributed dynamic programming for path planning

#### 2.1.1 - Asynchronous dynamic programming
- To discuss this, let's first consider the path planning problem
	- The path planning problem consists of:
		- A set of nodes $N$, comprising of $n$ nodes.
		- A set of directed links between nodes $L$
		- A weight function $w : L \rightarrow \mathbb{R}^+$
			- Meaning each link has a weight
		- Two nodes $s,t \in N$
		- The goal is to find the path from $s$ to $t$ with the lowest total weight
	- Also think about it like this:
		- Consider a set of goal nodes $T \subset N$. We are interested in the shortest path from $s$ to any node $t \in T$
	- If we have a node $x$ on the shortest path between $s$ and $t$, then the section between $s$ and $x$ must also be the shortest path for them.
		- Allows us to execute *dynamic programming*: "divide-and-conquer" style
- Represent the shortest distance from node $i$ to goal node $t$ as $h^*(i)$
	- $h^*(i)$ = value function of actual shortest path to goal $t$
	- $h(i)$ = value function of node $i$'s current estimate of shortest path to goal $t$
- Consider: AsyncDP alg
	- Per node $i$, the shortest distance from $i$ to $t$ can be determined by :
		- For each neighbor $j$:
			- Add their current $h(j)$ and our weight $w(i,j)$. Set my output value for $f(j)$ to this.
		- $h(i)$ is $min_jf(j)$
	- This doesn't scale well - since it's worst case O(n) time, having huge search spaces doesn't really work.
		- Time to use heuristics!

#### 2.1.2 - Learning real-time A* (LRTA*)
- Let's start with one agent. Worst case.
	- Our current node is $i$. Set this to $s$ (start node) when initializing.
	- Until we reach $t$:
		- For each neighbor $j$:
			- Add their current $h(j)$ and our weight $w(i,j)$. Set my output value for $f(j)$ to this.
				- *Initially, $h(j)$ will be 0. As trials take place, this will only grow.*
		- Get the node with the smallest estimated path $arg min_jf(j)$. Set *i'* to this.
		- Before moving, let's decide if we need to update and grow our current estimate. $h(i) \leftarrow max(h(i), f(i'))$
			- "If the newly estimated shortest path through $j$ called $f(i')$ is greater than the old estimated path to $h(i)$, update it." 
	- Think like this: 
		- We find the initial shortest path.
		- Run through more trials. If there's any legitimate reason to try out another path, take it.
		- Continue until we don't try out any other paths - until we repeat the same path twice in a row.
			- We don't need to try every single path!
- LRTA* has some assumptions:
	- Initialize node $h(i)$ to 0
	- Each node has at least one path to $t$
- LRTA* has some properties:
	- $h$ values never decrease and remain *admissible* (never more than $h*(i)$)
		- Ensure this by initializing nodes to 0 and only adding when a new shortest local path has been found.
	- LRTA* always terminates. Also, each successful completion of a pass of LRTA* is called a trial.
	- Repeating trials of LRTA* will converge on the shortest path when maintaining $h$-values.
	- If LRTA* finds the same shortest path on two sequential runs, this is the shortest path.
- LRTA* is a centralized algorithm, but can be executed (and sped up) with multiple agents. See book example in Figure 2.5
	- This is especially helpful to break up ties.


## APPENDIX C - MARKOV DECISION PROBLEMS (MDPs)

### C.1 - The model
- "A Markov Decision Problem (MDP) is a model for decision making in a dynamic, uncertain world."
- An MDP is a tuple $(S,A,p,r)$
	- $S$ - Set of states
	- $A$ - Set of actions
	- $p$ - Function $p : S \times A \times S \mapsto \mathbb{R}$ which specifies the *state transition probability* among states.
		- $p(s,a,s')$ - "When I am at state $s$ and I take action $a$, what is the probability that I end up in state $s'$?
	- $r$ - Function $r : S \times A \mapsto \mathbb{R}$ which specifies the reward given by taking an action $a$ while in state $s$
		- Rewards are aggregated in two ways
			- Limit-average reward
				- $\lim^\infty_{T=1}\frac{\sum^T_{t=1}r^(t)}{T}$
					- "$r^(t)$ is the reward you get at time step $t$."
					- "Average out all the rewards you get over the span of performing $T$ total time steps"
					- "As T grows, the average converges on a certain value."
			- Future-discounted reward
				- $\sum^{\infty}_{t=1}\beta^t r^{t}$
					- "No averaging this time - we're just summing all the reward values"
					- "However, we're multiplying each value by a coefficient raised to the power of the time step."
						- Coefficient is $\beta^t$, where $0 < \beta < 1$
					- "This means that as the time step gets larger, $\beta^t$ grows exponentially smaller - converging on 0"
						- Also meaning that the future-discounted reward is a finite sum!
					- "When $\beta \rightarrow 0$, future rewards are more heavily discounted - prioritizing short-term reward"
					- "When $\beta \rightarrow 1$, future rewards are less heavily discounted - prioritizing long-term reward"
- A (stationary, deterministic) policy $\pi : S \rightarrow A$ maps each state to an action.
- For the future examples, we'll use future-discounted reward

### 2.2 - Solving known MDPs via value iteration
- Each policy yields a total reward under each reward aggregation scheme.
- A policy that maximizes total reward is called an *optimal policy*
	- Generally, the algorithm to find an optimal policy is the computational task at hand
- While linear programming can solve MDPs in polynomial time, let's instead use *value iteration* for two reasons
	- LP-formulation of MDP is often too large to compute realistically, despite being in polynomial time
		- Value iteration is the basis of more practical solutions, including approximations of very large MDPs
	- Value iteration is relevant to the discussion of learning in MDPs
- Value iteration (VI) operates as such:
	- VI defines a value function $V^{\pi} : S \rightarrow \mathbb{R}$
		- $V^{\pi}(s) = Q^{\pi}(s, \pi(s))$
			- Meaning given a state $S$, find the value of acting according to policy $\pi$
			- In my mind, this is "naive." We only care about following the exact action prescribed in the policy.
	- VI also defines a state-action function $Q^{\pi} : S \times A \rightarrow \mathbb{R}$
		- $Q^{\pi}(s,a) = r(s,a) + \beta \sum_{\hat{s}} p(s,a,\hat{s})V^{\pi}(\hat{S})$
		- Meaning given a state $s$ and an action $a$, find the value of starting in $s$, taking action $a$, and continuing according to policy $\pi$
	- $V$ gives us the value of following a policy action at a state. $Q$ gives us the value of taking an action at a state, plus the values of all following states.
- Instead of the naive equations, imagine that we have an optimal policy $\pi^*$
	- The second equation would become $V^{\pi^*}(s) = \arg_a \max Q^{\pi^*}(s,a)$
		- Meaning that instead of choosing based on a state-action hardcoded by the policy, choose the action for which $Q^{\pi^*}(s,a)$ yields the greatest reward.
	- When working with $V^{\pi^*}$ and $Q^{\pi&*}$, the equations are referred to as the *Bellman equations*
	- The Bellman equations also give us a procedure for calculating the Q and V values of the optimal policy - ergo, they give us the optimal policy itself.
	- Consider:
		- $Q_{t+1}(s,a) \leftarrow r(s,a) + \beta \sum_{\hat{s}} p(s,a,\hat{s}V_t(\hat{s}))$
		- $V_t(s) \leftarrow \arg_a \max Q_t(s,a)$
			- **THIS RETURNS THE OPTIMAL ACTION**
	- Given an MDP and initializing Q values to an arbitrary value, repeatedly iterate the two sets of assignment operators.
	- After some set amount of time, Q and V values converge on an optimal policy.
	- What the heck does this even mean?
		- For $Q_{t+1}(s,a)$:
			- Every state is paired with every action. This tuple is assigned a value which eventually converges on the total (current + future reward) of taking action $a$ in state $s$.
		- For $V_t(s)$:
			- Every state is assigned an action which is the most optimal (meaning it has the largest total reward.) This is done by calculating $Q_{t+1}(s,a)$ for each $a$ and setting $V_t(s)$ to the highest yielding $a$
- In the real world, it's not so simple as performing these things.
	- The MDP may not be fully known beforehand, requiring learning.
	- The MDP may be too large to iterate over all instances of the equation.
		- Example: instead of a single value, states are represented as feature values where a state can take on many values.
			- This would result in exponential state number - 2 states can be `[[], [a], [b], [a,b]]` - 4 states.
			- We'd like to solve this in time polynomial to the number of features
		- To solve this, exploit independence properties.
			- Maybe this is like the subgames?
	- We discuss something similar to the above problem, but instead dealing with modularity of *actions* rather than states.
		- In a *multiagent MDP* (think each node calculating for itself), any action $a$ is really a vector of local actions $(a_1, a_2, ..., a_n)$.
			- This means that the number of global actions is exponential to the number of agents.
			- **But why???? What makes these actions different? It's not like they are $Q$ or $V$ values? Need to ask.**
				- **ANSWER**: Each "action" is a vector of local actions by each of $n$ agents. This means that each agent moves simultaneously. $A = A_1 * A_2$
- Let us consider a further subproblem
	- Suppose that the $Q$ values for the optimal policy are already computed. How hard is it to decide the action each agent should take?
	- In Appendix C, we mention that once an optimal policy has been converged on, we recover the optimal action in state $s$ with $argmax_a Q^{\pi^*} (s,a)$
		- This is "easy" to do with single agent. $A^n$ where $n = 1$ is linear. 
		- This is "hard" to do with multiple agents. $A^n$ where $n > 1$ is exponential time, so as $n$ grows, choosing the max $a$ takes exponentially longer.
		- Can we do better?
			- Generally, no. But interaction among agent actions can be quite limited.
			- We can exploit this. Think about calculating overall Q for multiple agents as calculating the Q (total reward) for each agent $i$ for each joint action in $A$, then adding all those indivual Q values up. Pretty intuitive.
			- Our overall Q function becomes $Q(s,a) = \sum^n_{i=1}Q_i (s,a)$
				- Our maximization problem becomes the max value of this new sum. We're finding the joint action which gives us the greatest joint total reward.
				- $\arg_a \max \sum^n_{i=1}Q_i (s,a)$ 
			- This doesn't solve our $A^n$ problem, butgenerally speaking, actions can be treated as discrete and noneffective on one another.
					- "Each $Q_i$ depends only on a small subset of variables"
		- Example of "Q subdivision": metal reprocessing plant.
			- In -> Station 1 (Load and Unload) -> Station 2 (Clean) -> Station 3 (Process) -> Station 4 (Eliminate Waste) -> Station 1 (Circular) -> Out
			- Each station is an agent.
			- Each agent can choose to send more material to the next station, or to suspend flow. ("Pass" or "Suspend")
			- We can reason that each station chooses actions based on the station after it - "downstream."
				- As Station 3, I'm not going to send metal to Station 4 if I know it can't handle the new load.
			- Therefore, the global $Q$ function becomes $Q(a_1, a_2, a_3, a_4) = Q_1(a_1, a_2) + Q_2(a_2, a_3) + Q_3(a_3, a_4) + Q_4(a_4, a_1)$
				- We want to compute the max of the the global Q. More specifically, we want to find the actions $a_1, a_2, a_3, a_4$ which yield the greatest $Q$.
					- $\arg_{a_1, a_2, a_3, a_4} \max Q(a_1, a_2, a_3, a_4) = Q_1(a_1, a_2) + Q_2(a_2, a_3) + Q_3(a_3, a_4) + Q_4(a_4, a_1)$
				- Here, we can employ a *variable elimination* algorithm, where we optimize one agent at a time. 
					- Break the $\arg \max$ into parts. Let's start with agent 4.
						- $\max_{a_1, a_2, a_3} Q_1(a_1, a_2) + Q_2(a_2, a_3) + \max_{a_4} [Q_3(a_3, a_4) + Q_4(a_4, a_1)]$
						- "I want to find the only local $Q_i$ functions where $a_4$ is used, then group them and mark them as depending on $a_1$ and $a_3$"
							- "I can find the optimal value of $a_4$ if I am given the optimal values of $a_1$ and $a_3$"
					- Represent this internal $\max$ expression with a new function $e_4(A_2, A_3)$. This gives us the optimal action $a_4^*$ based on $a_1^*$ and $a_3^*$
						- $e_4(a_3,a_1) = max_{a_4}[Q_3(a_3,a_4) + Q_4(a_4,a_1)]$
					- Substitute this into our original global function
						- $\max_{a_1, a_2, a_3} Q_1(a_1, a_2) + Q_2(a_2, a_3) + e_4(a_3, a_1)$
					- We have now "eliminated" agent 4. Rather, we know that we can calculate $a_4$ given the other actions. Repeat this process.
						- $e_3(a_2,a_1) = max_{a_3}[Q_2(a_2,a_3) + e_4(a_3,a_1)]$
						- $\max_{a_1, a_2} Q_1(a_1, a_2) + e_3(a_2, a_1)$
						- $e_2(a_1) = max_{a_2}[Q_2(a_1,a_2) + e_3(a_2,a_1)]$						
						- $e_1 = \max_{a_1} e_2(a_1)$
					- With this, we calculate each optimal action in order and use it in the next step.
						- Calculate $a_1^*$ with $\arg_{a_1} \max e_2 (a_1)$
						- **ASK ABOUT THIS. DOESN'T MAKE SENSE**
			- We can implement this procedure in a number of ways. You're still taking an exponential pass over agents, but it ends up being fewer operations.
		
### 2.3 - Negotiation, auctions, and optimization
- Here, we discuss distributed problem solving in an economic context.

#### 2.3.1 - From contract nets to auction-like optimization
- Here's our scenario.
	- Imagine we have a problem. This problem is too large to do by a single agent, so we split it up into smaller parts and distribute it among agents.
		- For each agent $i$, there is a function $c_i$
		- For any set of tasks $T$, $c_i(T)$ is the cost that agent $i$ incurs when completing **all the tasks** in $T$.
		- Each agent has different capabilities ("strengths and weaknesses.")
			- Therefore, certain tasks will be easier (less costly) while others will be "harder" (more costly)
		- At the beginning of the problem, each agent starts out with some set of subtasks 
			- We assume that this is not optimal and therefore needs adjusting.
				- "Not optimal" = the sum of all agent's costs are not the lowest
		- Agents then enter a "negotiation process" which improves on their assignment, culminating in optimality
			- This process can have an "anytime property" - even if interrupted prematurely, the resulting assignment is still more optimal than the initial assignment.
		- Negotiation consists of:
			- Agents contracting out assignments among themselves
				- Each "contract" consists of exchanging tasks *and money*
	- For the previous algorithm, we can imagine a scenario where an agent "bids" their cost (or marginal cost) on an assignment and the "owning agent" gives it to the lowest bidder, repeating.
		- This generally requires agents to enter "money-losing contracts." But there are other contract types which don't require money losing!
			- "Cluster contracts" - contracting for a bundle of tasks
			- "Swap contracts" - a swap of two tasks between agents
			- "Multi-agent contracts" - simultaneous transfers among many agents.
- Three questions arise.
	- How can we minnimize costs of subproblems if there's a monolithic larger problem to be dealt with?	
		- To be discussed later
	- When/how do agents actually make offers?
		- Through predetermined negotiation scehemes. In this case, we're looking at auctions.
			- Each scheme consists of:
				- "Bidding rules" - Permissible ways of making offers
				- "Market clearing rules" - definition of the outcome based on offers
				- "Information dissemination rules" - information made available to agents throughout the process
			- Our auction has a explicit centralized "auctioneer"
	- Since we're in a cooperative setting, why is "money-losing" relevant?
		- Auctions in particular are a means of allocating scarce resources among *self-interested agents*
			- "Self-interested" = game theoretic
			- Our discussion of contract-nets has some similarities to the game-theoretic auction but does not apply directly.
- We'll discussing:
	- A linear program (LP) to address the problem of *weighted matching in a bipartite graph* - AKA the *assignment problem*
	- An integer program (IP) to address the problem of *scheduling*

#### 2.3.2 - The assignment problem and linear programming

- The assignment problem, or "problem of weighted matching in a bipartite graph", goes as follows:
	- A (symmetric) assignment problem consists of:
		- A set $X$ of $n$ objects
		- A set $N$ of $n$ agents
		- A set $M \subseteq N \times X$ of possible assignment pairs
		- A function $v : M \rightarrow \mathbb{R}$ giving the value of each assignment pair
	- An assignment is a set of pairs $S \subseteq M$ such that each agent $i \in N$ and each object $j \in X$ are in at most one pair in $S$
		- Meaning an agent or object can only be in up to one pair at once
	- A *feasible assignment* is one where all agents are assigned an object
	- A feasible assignment $S$ is *optimal* if it maximizes $\sum_{(i,j) \in S} v(i,j)$
		- "... if it maximizes the sum of the pair values"
		- Note that we're trying to maximize here - not minimize like in the previously mentioned contract-net example.
- See the book example. The solution is obvious in smaller problems, but in larger problems, this isn't the case.
	- So how do we find the optimal assignment algorithmically?
- First, encode the problem as a linear problem.
	- Specifically, we can make a matrix to indicate assignment, or "assignment matrix"
		- When pair $(i,j)$ is selected, $x_{i,j} = 1$. Otherwise, $x_{i,j} = 0$
	- We use this to "activate" or "deactivate" values. Now, we can express the linear program as follows:
		- Maximize $\sum_{(i,j) \in M} v(i,j) x_{i,j}$ ...
		- Subject to:
			- $\sum_{j | (i,j) \in M} x_{i,j} \leq 1 \forall i \in N$
				- Each $i$ can only be in up to one pair at once
			- $\sum_{i | (i,j) \in M} x_{i,j} \leq 1 \forall j \in X$
				- Each $j$ can only be in up to one pair at once
			- If you examine the equations, you can tell they allow for fractional matches - continuous. but we can solve this integrally. 
- **Lemma 2.3.2**: *The LP encoding of the assignment problem has a solution such that for eery $i,j$ it is the case that $x_{i,j} = 0$ or $x_{i,j} = 1$. Furthermore, any optimal fractional solution can be converted in polynomial time to an optimal integral solution.*
- Since any LP can be solved in polynomial time, **Corollary 2.3.3**: *The assignment problem can be solved in polynomial time*
	- This doesn't solve our problems, though. 
		- The polynomial time solution is $O(n^3)$
		- The solution isn't parallelizable.
		- If one of the parameters changes, we have to do the whole calculation over again.
	- Let's instead explore the economic principle of competitive equilibrium
		- Imagine that:
			- Each $j \in X$ has an associated price
			- The price vector is $p = (p_1, \cdots, p_n)$ where $p_j$ is the price of object $j$
		- ..., given:
			- An assignment $S \subseteq M$ 
			- A price vector $p$
		- ..., define the "utility" from an assignment $j$ to agent $i$ as $u(i,j) = v(i,j) - p_j$
			- Think of $p_j$ as the value cost to acquire $j$. 
			- Think of $v(i,j)$ as the value provided by object $j$ to agent $i$ once acquired.
		- An assignment and a set of prices are in *competitive equilibrium* when each agent is assigned the object that maximizes their utility given current prices.
		- Formally, *a feasible assignment $S$ and a price vector $p$ are in *competitive equilibrium* when, for every pairing $(i,j) \in S$, it is the case that $\forall k, u(i,j) \geq u(i,k)$ *
- **Theorem 2.3.5**: *If a feasible assignment $S$ and a price vector $p$ satisfy the competitive equilibrium condition then $S$ is an optimal assignment. Furthermore, for any optimal solution S, there exists a price vector such that $p$ and $S$ satisfy the competitive equilibrium condition.*
	- What does this mean?
		- We have the values of each assignment pair. The value may be different per pair.
		- We also have a (universal) price for each object, stored in a vector
			- An agent will "spend" some amount to "earn" some amount as payoff. This "utility" (marginal gain/loss) for the pair is $u(i,j)$
		- We're aiming to create an assignment pair for each agent. This set of assignment pairs is $S$.
		- Given an assignment $S$ and a price vector $p$, if each pair in the assignment cannot be improved by swapping objects between agents, then $S$ is optimal.
		- We can also see this the other way around. If we know $S$ is optimal, we know there's a $p$ such that the two together satisfy the competitive equilibrium condition.
	- What are the implications?
		- We can "search the space of competitive equilibria" (increment or iterate through versions of pairings and $S$) to find equilibrium (and therefore optimality)
			- One way to do this is with auction-like procedures where agents "bid" in a predetermined way.
			- We will look at "open outcry" (ascending auction-like procedures)
				- But only after we discuss optimization and competitive equilibrium.
- To understand why CE applies to optimization, let's look at a general form of our LP.
	- Maximize $\sum^n_{i=1} c_ix_i$
	- Subject to:
		- $\sum^n_{i=1} a_{i,j}x_i \leq b_j \forall j \in {1, ..., m}$
		- $x_i \geq 0 \forall i \in {1, ..., n}$
	- What does this mean?
		- These are the parts we discussed earlier for the linear program
		
## CHAPTER 3 - NONCOOPERATIVE GAME THEORY

- So far, we've only had cooperative situations. What about self-interested agents?
- We model an agent's interests with a *utility function.*
	- A mapping from the natural world to real numbers

### 3.1 - Self-interested agents

- This means the agent is interested in maximizing its own utility - not necessarily for good/bad things to happen to others.

### 3.1.1 - Friends and enemies

- See movie example (BOTS flavored)

### 3.1.2 - Preferences and utility

- Why is it alright to model as utility? The real world is much more complex
	- Utility theorists would say that utility represents *preference*. We often look to von Neumann and Morgenstein's equations.
		- Let $O$ denote a finite set of outcomes for an agent.
		- For any pair $o_1, o_2 \in O$:
			- Let $o_1 \succeq o_2$ denote that the agent **weakly prefers** $o_1$ to $o_2$
			- Let $o_1 \sim o_2$ denote that the agent **is indifferent** between $o_1$ and $o_2$
				- We can also write this as $o_1 \succeq o_2 \wedge o_2 \succeq o_1$
			- Let $o_1 \succ o_2$ denote that the agent **strictly prefers** $o_1$ to $o_2$
				- We can also write this as $o_1 \succeq o_2 \wedge \neg (o_2 \succeq o_1)$
	- There may be some uncertainty with how outcomes are selected. This is codified through **lotteries**
		- A lottery is the random selection of one of a set of outcomes according to specified probabilities.
		- i.e. a lottery is a probability distribution over outcomes written $[p_1 : o_1 ,..., p_k : o_k]$ where:
			- $o_i in O$
			- $p_i \geq 0$
			- $\sum^k_{i=1} p_1 = 1$ - all probabilities add to 1
		- Let $\mathcal{L}$ denote the set of all lotteries.
			- With this set, we can extend the $\succeq$ relation to apply to lotteries as well.
				- Meaning that an agent may strictly prefer/weakly prefer/be indifferent to one lottery over another
- **Axioms of utility theory**
	- **Axiom 3.1.1 - Completeness**
		- $\forall o_1, o_2, o_1 \succ o_2 or o_2 \succ o_1 or o_1 \sim o_2$
		- This tells us that there is an order in preference, with ties allowed
		- For every pair, the agent either prefers one over the other, or is indifferent.
	- **Axiom 3.1.2 - Transitivity**
		- $If \o_1 \succeq o_2 \wedge \o_2 \succeq o_3, then o_1 \succeq o_3$
		- There is an order between preferring one pair and another with overlapping elements. Otherwise, you could have infinite loss.
			- Consider the money pump problem
	- **Axiom 3.1.3 - Substitutability**
		- $If o_1 \sim o_2, then for all sequences of one or more outcomes o_3,...,o_k and sets of probabilities p, p_3,...,p_k for which p + \sum^k_{i=3} p_i = 1, [p:o_1, p_3:o_3,...,p_k:o_k] \sim [p:o_1, p_3:o_3,...,p_k:o_k]$
			- $P_{\mathcal{l}}(o_i)$ denotes the probability that outcome $o_i$ is selected by lottery $\mathcal{l}$
		- If an agent is indifferent between two outcomes, they are indifferent between lotteries that only vary in that outcome.
		- Lotteries can be nested inside one another, too. See book example.
	- **Axiom 3.1.4 - Decomposability**
		- $If \forall o_i \in O, P_{\mathcal{l}_1}(o_i) = P_{\mathcal{l}_2}(o_i) then \mathcal{l}_1 \sim \mathcal{l}_2$ 
		- An agent is always indifferent between lotteries which induce the same probabilities over outcomes, regardless of whether lotteries are single or nested.
		- Also sometimes called the *"no fun in gambling"* axiom because it implies that rolling the dice more times has no impact on preference.
	- **Axiom 3.1.5 - Monotonicity**
		- $If o_1 \succ o_2 and p > q then [p:o_1, 1-p:o_2] \succ [q:o_1, 1-q:o_2]$
		- If an outcome is preferred over another, the agent will prefer the lottery that gives more probability to that first outcome.
		- Called monotonicity because the value of the probability isn't important - just the relationship between lotteries
	- **Lemma 3.1.6**
		- $If a preference relation \succeq satisfies the axioms of$
			- $completeness$ (where all outcome pairs have $\succ, \prec, or \sim$ relations),
			- $transitivity$ (where preference forms an order between overlapping pairs),
			- $decomposability$ (where identical lotteries are indifferent regardless of form), and 
			- $monotonicity$ (where lotteries giving more of a preferred outcome are preferred), 
		- $... and if...$
			- $o_1 \succ o_2$
			- $o_2 \succ o_3$
		- $... then there exists some probability p such that for all p' < p, o_2 \succ [p':o_1, (1-p'):o_3]$
			- "If $o_1 \succ o_2 \succ o_3$ and we abide by the rules, then there's some probability of getting $o_1$ where below that point, I would rather take $o_2$ instead of risking it for $o_3$"
		- $... and for all p'' > p, [p'':o_1, (1-p''):o_3] \succ o_2$
			- "... and above that point, I would rather risk it for $o_1$ instead of taking a guaranteed $o_2$"
		- See the proof!
	- **Axiom 3.1.7 - Continuity**
		- $If o_1 \succ \o_2 and o_2 \succ o_3, \exists p \in [0,1] such that o_2 \sim [p:o_1 , (1-p):0_3]
	- If we accept completeness, transitivity, decomposability, monotonicity, and continuity, we must accept the existance of utility functions.
		- This is the following theoreom:
	- **Theoreom 3.1.8 - von Neumann and Morgenstern**
		- $If a preference relation satisfies completeness, transitivity, decomposability, monotonicity, and continuity:$
		- $... there exists a function u : \mathcal{L} such that:$
			- $u(o_1) \geq u(o_2) iff o_1 \succeq o_2, and $
				- "The utility of a preferred outcome is greater than the utility of a nonpreferred outcome in a pair"
			- $u([p:o_1,...,p_k:o_k]) = \sum^k_{i=1}p_iu(o_i)$	
				- "The utility of a lottery is the sum of (each outcome times its probability)"
			- See proof!
	- Why not use real values instead of utility?
		- Because utility and direct material payoff aren't always linearly related.
	- What if we don't want our utility to merely fit within $[0,1]$?
		- We can write a secondary utility function that transforms it - think $u'(o) = au(o) + b$ is also a utility function for an agent, so long as a and b are constant and a is positive.
### 3.2 - Games in normal form
